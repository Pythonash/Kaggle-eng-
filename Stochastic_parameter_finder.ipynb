{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Stochastic parameter finder.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pythonash/Kaggle-eng-/blob/Brain/Stochastic_parameter_finder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This notebook is written by pythonash.\n",
        "\n",
        "I was meant to find the proper parameter containing learning rate, dropout rate, and so on.\n",
        "\n",
        "This notebook will be modified until either I finally get optimal structure or this competition is ended with my indifference due to my work.\n",
        "\n",
        "The purpose of notebook is to gather the loss result when parameters are changed as much as possible my time allows.\n",
        "\n",
        "From the combination with many parameters, you can find the optimal parameter or find the way you can optimize your model.\n",
        "\n",
        "Let's Start!"
      ],
      "metadata": {
        "id": "KD6gBcwVQS7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "o9fxtxPj06Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# import ubiquant\n",
        "from sklearn.model_selection import KFold\n",
        "import random\n",
        "import time\n",
        "\n",
        "start = time.time()"
      ],
      "metadata": {
        "papermill": {
          "duration": 5.150271,
          "end_time": "2022-02-17T06:49:49.868958",
          "exception": false,
          "start_time": "2022-02-17T06:49:44.718687",
          "status": "completed"
        },
        "tags": [],
        "id": "7IDIXQEyPNZs",
        "execution": {
          "iopub.status.busy": "2022-03-06T04:55:50.439247Z",
          "iopub.execute_input": "2022-03-06T04:55:50.439949Z",
          "iopub.status.idle": "2022-03-06T04:55:55.598092Z",
          "shell.execute_reply.started": "2022-03-06T04:55:50.439851Z",
          "shell.execute_reply": "2022-03-06T04:55:55.597275Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and handle dataset\n",
        "\n",
        "- This handling procedure is introduced in my previous notebook [End to end simple and powerful DNN with LeakyReLU](https://www.kaggle.com/pythonash/end-to-end-simple-and-powerful-dnn-with-leakyrelu).\n",
        "\n",
        "- If you need more information about dataset handling, please check my notebook.\n",
        "\n",
        "\n",
        "## NB\n",
        "\n",
        "Note that I little bit change the procedure.\n",
        "\n",
        "In previous notebook, I used \"investment_id\" and \"Standard scaling\".\n",
        "\n",
        "But, I won't use both things in this notebook.\n",
        "\n",
        "Furthermore, I will change the \"f_col\" values into normalization (Min-Max)."
      ],
      "metadata": {
        "id": "rj-xHp6Z06L1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet('../input/ubiquant-parquet/train_low_mem.parquet')\n",
        "df"
      ],
      "metadata": {
        "papermill": {
          "duration": 42.185531,
          "end_time": "2022-02-17T06:50:32.211548",
          "exception": false,
          "start_time": "2022-02-17T06:49:50.026017",
          "status": "completed"
        },
        "tags": [],
        "id": "zvgY3iirPNZ0",
        "execution": {
          "iopub.status.busy": "2022-03-06T04:55:55.599696Z",
          "iopub.execute_input": "2022-03-06T04:55:55.600806Z",
          "iopub.status.idle": "2022-03-06T04:56:35.69829Z",
          "shell.execute_reply.started": "2022-03-06T04:55:55.600766Z",
          "shell.execute_reply": "2022-03-06T04:56:35.697549Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_col = df.drop(['row_id','time_id','investment_id','target'],axis=1).columns\n",
        "f_col"
      ],
      "metadata": {
        "papermill": {
          "duration": 2.06051,
          "end_time": "2022-02-17T06:50:36.991602",
          "exception": false,
          "start_time": "2022-02-17T06:50:34.931092",
          "status": "completed"
        },
        "tags": [],
        "id": "7HU8RHAUPNZ2",
        "execution": {
          "iopub.status.busy": "2022-03-06T04:56:38.057228Z",
          "iopub.execute_input": "2022-03-06T04:56:38.057673Z",
          "iopub.status.idle": "2022-03-06T04:56:40.550998Z",
          "shell.execute_reply.started": "2022-03-06T04:56:38.057605Z",
          "shell.execute_reply": "2022-03-06T04:56:40.550322Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(pd.DataFrame(df[f_col]))"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.095234,
          "end_time": "2022-02-17T06:50:37.176892",
          "exception": false,
          "start_time": "2022-02-17T06:50:37.081658",
          "status": "completed"
        },
        "tags": [],
        "id": "Hl3uby-UPNZ2",
        "execution": {
          "iopub.status.busy": "2022-03-06T04:56:40.552363Z",
          "iopub.execute_input": "2022-03-06T04:56:40.552789Z",
          "iopub.status.idle": "2022-03-06T04:56:44.415716Z",
          "shell.execute_reply.started": "2022-03-06T04:56:40.552744Z",
          "shell.execute_reply": "2022-03-06T04:56:44.415052Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataset(df):\n",
        "    f_df = df[f_col]\n",
        "    scaled_f = scaler.transform(pd.DataFrame(f_df))\n",
        "    data_x = pd.DataFrame(scaled_f)\n",
        "    data_x.columns = f_df.columns\n",
        "    del f_df\n",
        "    data_x = data_x.astype('float16')\n",
        "    return data_x"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.037193,
          "end_time": "2022-02-17T06:50:37.308703",
          "exception": false,
          "start_time": "2022-02-17T06:50:37.27151",
          "status": "completed"
        },
        "tags": [],
        "id": "4lRT8668PNZ3",
        "execution": {
          "iopub.status.busy": "2022-03-06T04:56:44.416927Z",
          "iopub.execute_input": "2022-03-06T04:56:44.417696Z",
          "iopub.status.idle": "2022-03-06T04:56:44.423102Z",
          "shell.execute_reply.started": "2022-03-06T04:56:44.417656Z",
          "shell.execute_reply": "2022-03-06T04:56:44.422258Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df=df.astype('float16')\n",
        "df_x = make_dataset(df)\n",
        "df_x"
      ],
      "metadata": {
        "papermill": {
          "duration": 12.385962,
          "end_time": "2022-02-17T06:50:49.844364",
          "exception": false,
          "start_time": "2022-02-17T06:50:37.458402",
          "status": "completed"
        },
        "tags": [],
        "id": "kdtoVGMLPNZ3",
        "execution": {
          "iopub.status.busy": "2022-03-06T04:56:44.425971Z",
          "iopub.execute_input": "2022-03-06T04:56:44.426209Z",
          "iopub.status.idle": "2022-03-06T04:56:55.575515Z",
          "shell.execute_reply.started": "2022-03-06T04:56:44.426172Z",
          "shell.execute_reply": "2022-03-06T04:56:55.573649Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_y = pd.DataFrame(df['target'])\n",
        "df_y"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.044144,
          "end_time": "2022-02-17T06:50:49.980591",
          "exception": false,
          "start_time": "2022-02-17T06:50:49.936447",
          "status": "completed"
        },
        "tags": [],
        "id": "hfSW4HVfPNZ3",
        "execution": {
          "iopub.status.busy": "2022-03-06T04:56:55.57674Z",
          "iopub.execute_input": "2022-03-06T04:56:55.577064Z",
          "iopub.status.idle": "2022-03-06T04:56:55.593012Z",
          "shell.execute_reply.started": "2022-03-06T04:56:55.577026Z",
          "shell.execute_reply": "2022-03-06T04:56:55.592346Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del df"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.092862,
          "end_time": "2022-02-17T06:50:50.169638",
          "exception": false,
          "start_time": "2022-02-17T06:50:50.076776",
          "status": "completed"
        },
        "tags": [],
        "id": "2vHrSQ3nPNZ4",
        "execution": {
          "iopub.status.busy": "2022-03-06T04:56:55.594328Z",
          "iopub.execute_input": "2022-03-06T04:56:55.594604Z",
          "iopub.status.idle": "2022-03-06T04:56:55.598252Z",
          "shell.execute_reply.started": "2022-03-06T04:56:55.594568Z",
          "shell.execute_reply": "2022-03-06T04:56:55.597541Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stochastic model structure\n",
        "\n",
        "- I construct my model with randomness (i.e., the number of neurons, dropout rate, and learning rate will be assigned randomly).\n",
        "\n",
        "- I used PReLU instead of LeakyReLU.\n",
        "\n",
        "- For every iteration, you will get random neurons, dropout rate, and learning rate in your model.\n",
        "\n",
        "- By doing so, you will gather the logs, which are effects of parameters on loss result.\n",
        "\n",
        "You can change model structure as much as you want."
      ],
      "metadata": {
        "id": "obM6qNCI06L4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pythonash_model():\n",
        "    neurons = random.randint(16, 513)\n",
        "    drop_rate = random.randint(1,6)/10\n",
        "    lr_rate = random.uniform(0.0005, 0.007)\n",
        "    \n",
        "    inputs_ = tf.keras.Input(shape = [df_x.shape[1]])\n",
        "    x = tf.keras.layers.Dense(neurons, kernel_initializer = 'he_normal')(inputs_)\n",
        "    batch = tf.keras.layers.BatchNormalization()(x)\n",
        "    leaky = tf.keras.layers.PReLU()(batch)\n",
        "    \n",
        "    x = tf.keras.layers.Dense(neurons, kernel_initializer = 'he_normal')(leaky)\n",
        "    batch = tf.keras.layers.BatchNormalization()(x)\n",
        "    leaky = tf.keras.layers.PReLU()(batch)\n",
        "    \n",
        "    x = tf.keras.layers.Dense(neurons, kernel_initializer = 'he_normal')(leaky)\n",
        "    batch = tf.keras.layers.BatchNormalization()(x)\n",
        "    leaky = tf.keras.layers.PReLU()(batch)\n",
        "    \n",
        "    x = tf.keras.layers.Dense(neurons, kernel_initializer = 'he_normal')(leaky)\n",
        "    batch = tf.keras.layers.BatchNormalization()(x)\n",
        "    leaky = tf.keras.layers.PReLU()(batch)\n",
        "    \n",
        "    x = tf.keras.layers.Dense(neurons, kernel_initializer = 'he_normal')(leaky)\n",
        "    batch = tf.keras.layers.BatchNormalization()(x)\n",
        "    leaky = tf.keras.layers.PReLU()(batch)\n",
        "    drop = tf.keras.layers.Dropout(drop_rate)(leaky)\n",
        "    \n",
        "    x = tf.keras.layers.Dense(neurons, kernel_initializer = 'he_normal')(drop)\n",
        "    batch = tf.keras.layers.BatchNormalization()(x)\n",
        "    leaky = tf.keras.layers.PReLU()(batch)\n",
        "    \n",
        "    x = tf.keras.layers.Dense(neurons, kernel_initializer = 'he_normal')(leaky)\n",
        "    batch = tf.keras.layers.BatchNormalization()(x)\n",
        "    leaky = tf.keras.layers.PReLU()(batch)\n",
        "    drop = tf.keras.layers.Dropout(drop_rate)(leaky)\n",
        "    \n",
        "    outputs_ = tf.keras.layers.Dense(1)(drop)\n",
        "    \n",
        "    model = tf.keras.Model(inputs = inputs_, outputs = outputs_)\n",
        "    \n",
        "    rmse = tf.keras.metrics.RootMeanSquaredError()\n",
        "\n",
        "    learning_sch = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate = lr_rate,\n",
        "    decay_steps = 10000,\n",
        "    decay_rate = 0.97)\n",
        "    \n",
        "    adam = tf.keras.optimizers.Adam(learning_rate = learning_sch)\n",
        "    \n",
        "    model.compile(loss = 'mse', metrics = rmse, optimizer = adam)\n",
        "    opt_name = str(model.optimizer).split('.')[3].split()[0]\n",
        "    print('Current set is \\n neurons: {0},\\n Drop rate: {1}, \\n learning_rate: {2}'.format(neurons, drop_rate, lr_rate))\n",
        "    \n",
        "    return neurons, drop_rate, lr_rate, model"
      ],
      "metadata": {
        "papermill": {
          "duration": 3.3428,
          "end_time": "2022-02-17T06:50:53.606125",
          "exception": false,
          "start_time": "2022-02-17T06:50:50.263325",
          "status": "completed"
        },
        "tags": [],
        "id": "2hKZ3clVPNZ4",
        "execution": {
          "iopub.status.busy": "2022-03-06T04:56:55.599839Z",
          "iopub.execute_input": "2022-03-06T04:56:55.600584Z",
          "iopub.status.idle": "2022-03-06T04:56:55.617901Z",
          "shell.execute_reply.started": "2022-03-06T04:56:55.600535Z",
          "shell.execute_reply": "2022-03-06T04:56:55.617228Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the simulation log\n",
        "\n",
        "For each iteration, the logs are generated.\n",
        "\n",
        "And this code save the logs automatically."
      ],
      "metadata": {
        "id": "Y2cGY3zK06L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simulation_log = []\n",
        "num_iter = 1\n",
        "for iteration in np.arange(1,100):\n",
        "    end = time.time()#\n",
        "    if round(end - start,0) /3600 < 5:#\n",
        "        print('Current running time: {} sec.'.format(round(end - start,0)))\n",
        "        num_fold = 1\n",
        "        kfold_generator = KFold(n_splits =5, shuffle=True)\n",
        "        callbacks = tf.keras.callbacks.ModelCheckpoint('pythonash_model.h5', save_best_only = True)\n",
        "        neurons, drop_rate, lr_rate, model = pythonash_model()\n",
        "        fold_model = model.save('fold_model.h5')\n",
        "        del fold_model\n",
        "        del model\n",
        "        for train_index, val_index in kfold_generator.split(df_x, df_y):\n",
        "            fold_model = tf.keras.models.load_model('fold_model.h5')\n",
        "            # Split training dataset.\n",
        "            train_x, train_y = df_x.iloc[train_index], df_y.iloc[train_index]\n",
        "            # Split validation dataset.\n",
        "            val_x, val_y = df_x.iloc[val_index], df_y.iloc[val_index]\n",
        "            # Make tensor dataset.\n",
        "            tf_train = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n",
        "            tf_val = tf.data.Dataset.from_tensor_slices((val_x, val_y)).shuffle(2022).batch(1024, drop_remainder=True).prefetch(1)\n",
        "            # Load model\n",
        "            ###############################################################################################        \n",
        "            print('======================================Fold %d Start!======================================='%num_fold)\n",
        "            fit_history = fold_model.fit(tf_train, callbacks = callbacks, epochs = 5, #### change the epochs into more numbers.\n",
        "                     validation_data = (tf_val), shuffle=True, verbose = 1)\n",
        "            min_loss = np.array(fit_history.history['val_loss']).min()\n",
        "            print('===========================================================================================')\n",
        "            print('Model achieves %f in validation set.' %min_loss)\n",
        "            print('===========================================================================================')\n",
        "            simulation_log.append([num_iter, num_fold, neurons, drop_rate, lr_rate, min_loss])\n",
        "            log_df = pd.DataFrame(simulation_log)\n",
        "            log_df.columns = ['num_iter','num_fold','neurons', 'drop_rate', 'lr_rate', 'min_loss']\n",
        "            print(log_df)\n",
        "            log_df.to_csv('./Parameter finder log.csv', encoding = 'utf-8-sig', index = False)\n",
        "            print('===========================================================================================')\n",
        "            # Delete tensor dataset and model for avoiding memory exploring.\n",
        "            del tf_train\n",
        "            del tf_val\n",
        "            del fit_history\n",
        "            del fold_model\n",
        "            num_fold += 1\n",
        "    else:\n",
        "        print('Memory using time is over.')\n",
        "        break\n",
        "#     del model\n",
        "    del neurons\n",
        "    del drop_rate\n",
        "    del lr_rate\n",
        "    del min_loss\n",
        "    print('%d iteraion is over.' %num_iter)\n",
        "    print('===========================================================================================')\n",
        "    num_iter+=1\n",
        "    "
      ],
      "metadata": {
        "id": "j05Pa_AnPNZ5",
        "execution": {
          "iopub.status.busy": "2022-03-06T04:56:55.619268Z",
          "iopub.execute_input": "2022-03-06T04:56:55.619796Z",
          "iopub.status.idle": "2022-03-06T05:08:37.680288Z",
          "shell.execute_reply.started": "2022-03-06T04:56:55.619756Z",
          "shell.execute_reply": "2022-03-06T05:08:37.679503Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}